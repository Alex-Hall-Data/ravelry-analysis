#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED
pca=prcomp(na.omit(processed_dataset), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
biplot(pca,scale=0)
biplot(pca)
class(pca)
View(head(processed_dataset))
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp(na.omit(processed_dataset[,2]), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp(na.omit(processed_dataset[,-2]), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
biplot(pca)
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp(rating_total~., data=na.omit(processed_dataset[,-2]), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
#perform PCR
#set.seed (57)
#pcr_model <- pcr(rating_total~., data = processed_dataset, scale = TRUE, validation = "CV")
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp(rating_total~., na.omit(processed_dataset), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
#perform PCR
#set.seed (57)
#pcr_model <- pcr(rating_total~., data = processed_dataset, scale = TRUE, validation = "CV")
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp( na.omit(processed_dataset[,-2]), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
#perform PCR
#set.seed (57)
#pcr_model <- pcr(rating_total~., data = processed_dataset, scale = TRUE, validation = "CV")
install.packages("pls")
library(pls)
#load in raw dataset
load("~/R/ravelry/data/processed_dataset.RData")
#convert list columns to vectors
attrs<-names(processed_dataset)
#first 17 columns are lists in original dataset
for( j in (1:17)){
#replace nulls with zero
attr_vector<-c(processed_dataset[,j])
for (i in(1:length(attr_vector))){
if(is.null(attr_vector[i][[1]])==TRUE){
attr_vector[i]<-0}
else{attr_vector[i]<-attr_vector[i]}
}
attr_vector<-unlist(attr_vector)
processed_dataset[,j]<-attr_vector
}
#================================================================================================
#Remove attributes with no info, or just text
#remove hook size attributes - hook size is completely dependant on needle size so gives no new info
drops<-grep('hook',attrs,TRUE)
processed_dataset<-processed_dataset[,-drops]
#remove attributes which are unlikely to carry any useful information (text attributes)
drops<-c("permalink","name","company_name","company_permalink")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove 'texture' and 'needle name' attributes. The former is text based. The latter are dependant on needle size
drops<-c("texture","max_needle_name","min_needle_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#yarn weight names could be converted to dummy variables, but this is pointless since it is dependant on ply - remove
drops<-c("yarn_weight_name")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#now delete all 'id' attributes - these contain no information
drops<-c("company_id","min_needle_id","max_needle_id","id","yarn_weight_id")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("rating_average","rating_count")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#most needle size attributes can be removed since they are just variatins of the same measurement (metric, imperial etc)
drops<-c("max_needle_pretty_metric","max_needle_crochet","max_needle_us","max_needle_us_steel",
"max_needle_knitting","min_needle_pretty_metric","min_needle_crochet","min_needle_us","min_needle_us_steel",
"min_needle_knitting","yarn_weight_min_gauge","yarn_weight_wpi","yarn_weight_max_gauge",
"yarn_weight_knit_gauge","yarn_weight_crochet_gauge")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#convert allattributes to
processed_dataset <- as.data.frame(sapply(processed_dataset, as.numeric))
#=================================================================================================
#Make new dependant attributes for fiber types (plant, animal or synthetic)
#create new columns for the new attributes
vegetable<-rep(0,nrow(processed_dataset))
animal<-rep(0,nrow(processed_dataset))
synthetic<-rep(0,nrow(processed_dataset))
#list attribute names
vegetable_names<-c("Cotton", "Linen", "Bamboo","Soy","Plant fiber","Hemp","Tencel")
animal_names<-c("Wool","Mohair","Merino","Silk","Angora","Cashmere","Alpaca","Llama",
"Camel","Bison","Yak","Qiviut")
synthetic_names<-c("Nylon","Rayon","Acryllic","Polyester","Metallic","Microfiber")
processed_dataset<-cbind(processed_dataset,vegetable,animal,synthetic)
#create totals (note no NAs since these were previously replaced with zeros)
processed_dataset$vegetable<-rowSums(processed_dataset[,names(processed_dataset) %in% vegetable_names])
processed_dataset$animal<-rowSums(processed_dataset[,names(processed_dataset) %in% animal_names])
processed_dataset$synthetic<-rowSums(processed_dataset[,names(processed_dataset) %in% synthetic_names])
#note, the 'other' attribute is for fibers of unknown type. May wish to consider this similarly to the above atributes
#=================================================================================================
#We are now left with 44 attributes for analysis
#remove thread size attribute as it is constant (no info and doesn't allow scaling)
#remove average rating and rating count since the relationship between these and overall rating is trivial
drops<-c("thread_size")
processed_dataset<-processed_dataset[ , !(names(processed_dataset) %in% drops)]
#standardise the data prior to any model fitting
#processed_dataset<-scale(processed_dataset)
set.seed(57)
#perform PCA (set scale=T to standardise dataset). NAS OMITTED (note, removed second column since this is the dependant variable)
pca=prcomp( na.omit(processed_dataset[,-2]), scale=TRUE)
#get varience of each principal component
pca.var<-pca$sdev^2
#get proportion of varience explained by each principal component
pca.pve=pca.var/sum(pca.var)
#plot proportion of varience explained by each PC:
barplot(pca.pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,max(pca.pve)))
#perform PCR
set.seed (57)
pcr_model <- pcr(rating_total~., data = na.omit(processed_dataset), scale = TRUE, validation = "CV")
summary(pcr_model)
validationplot(pcr_model,type="MSEP")
validationplot(pcr_model,val.type="MSEP")
?validationplot
validationplot(pcr_model,val.type="MSEP",legend=TRUE)
validationplot(pcr_model)
validationplot(pcr_model,val.type="MSEP")
